{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "神经网络初始化\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.权重初始化\n",
    "- 全零初始化:错误的初始化。因为如果全零初始化，那么所有的输出都是一样的，那么在反向传播中就会算出同样的梯度，从而进行同样的参数更新，那么神经元之间也就失去了不对称性的源头.\n",
    "- 小随机数初始化:权重要非常接近于0但是不能是0，解决办法就是生成小随机数，以此来打破对称性。方法就是$\\omega=0.01 * np.random.randn(D,H)$。$randn$函数是基于零均值和标准差的一个高斯分布来生成随机数，这样每个神经元的权重向量都被初始化为一个随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元的指向是随机的。但是这个方法有个问题，就是随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。那么我们可以使用$\\frac{1}{\\sqrt{n}}$来校准方差，也就是$\\omega=\\frac{np.random.randn(n)}{\\sqrt{n}}$，其中n是输入数据的数量，这样就保证了网络中所有神经元起始时有近似同样的输出分布，这样可以提高收敛速度\n",
    "- 对于ReLU神经元的特殊初始化:$\\omega=\\frac{np.random.randn(n)}{\\sqrt{\\frac{2}{n}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.稀疏初始化\n",
    "另一个处理非标定方差的方法是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接(其权重数值由一个小的好似分布生成)。一个比较典型的连接数目是10个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.偏置初始化\n",
    "通常将偏置初始化为0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.批量归一化\n",
    "其做法是让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。在实现层面，应用这个技巧通常意味着全连接层(或者卷积层)与激活函数之间添加一个BatchNorm层。<br/>\n",
    "![images](images/02_00_05_001.png)\n",
    "![images](images/02_00_05_002.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
