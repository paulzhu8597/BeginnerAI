{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "传统神经网络\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.神经网络的由来\n",
    "## 1.1.线性回归\n",
    "线性关系来描述输入到输出的映射关系，应用场景包括网络分析，银行风险分析，基金股价分析，天气预报，传统的感知机只能进行线性可分，对于线性不可分的情况就会无法收敛，比如异或问题，所以要引入非线性的因素。但是为何要引入非线性以及为何要引入非线性的激励函数，因为不使用激励函数的话，神经网络的每层都只是做线性变换，多层输入叠加后也还是线性变换。因为线性模型的表达能力不够，激励函数可以引入非线性因素\n",
    "- 目标方程:$y=ax_1+bx_2+cx_3+d$\n",
    "- 参数:$m=[a,b,c,d]$\n",
    "- 数据:$[(x_{11},x_{21},x_{31}),(x_{12},x_{22},x_{32}),...,(x_{1n},x_{2n},x_{3n})] [y_1,y_2,...,y_n]$\n",
    "- 预测:$\\hat{y_t}=ax_{1t}+bx_{2t}+cx_{3t}+d$\n",
    "- 目标:$minimize(\\hat{y_t}-y_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.从线性到非线性\n",
    "添加一个非线性的激励函数,在一个线性函数的外面，套上一个非线性的激励函数，从而实现一个非线性的拟合。神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是阶跃函数；而当我们说神经元时，激活函数往往选择为sigmoid函数或tanh函数。比如对于感知机中无法分割的异或问题，其实两条线就可以分开，那么我们其实可以这么考虑，给定两组权重，可以分别得到两条直线，然后两条之间做and操作，就可以分开。这就形成了一个简单的神经网络。选择激励函数的两个考量：\n",
    "- 正向对输入的调整\n",
    "- 反向梯度损失(反向传播算法，是神经网络的训练算法)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.神经网络构建\n",
    "从第一层神经网络到最终输出，每一个神经元的数值由前一层神经元数值，神经元参数$\\omega$，b以及激励函数共同决定第n+1层第k个神经元的方程可由公式表示为:\n",
    "$$Z_{n+1,k}=\\sum_{i=1}^mW_{n,k,i} \\bullet x_{n,i} + b_{n,k}, y_{n+1,k}=\\frac{1}{1+e^{-z_{n+1,k}}}$$\n",
    "在这里，m表示第n层神经网络的宽度，n为当前神经网络的深度.<br/>\n",
    "![images](images/02_00_03_001.png)<br/>\n",
    "这也是神经网络正向的计算<br/>\n",
    "![images](images/02_00_03_004.png)<br/>\n",
    "对于反向计算，则是求梯度.第n-1层的梯度，是用第n层的梯度除以参数得到的.所以真正计算的时候，都是从Loss出发，往前一步一步计算梯度。这就是链式法则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.传统神经网络的输出\n",
    "神经网络实际上就是一个输入向量$\\overrightarrow{x}$到输出向量$\\overrightarrow{y}$的函数，即：$\\overrightarrow{y}=f_{network}(\\overrightarrow{x})$,根据输入计算神经网络的输出，需要首先将输入向量$\\overrightarrow{x}$的每个元素$x_i$的值赋给神经网络的输入层的对应神经元，然后根据式1依次向前计算每一层的每个神经元的值，直到最后一层输出层的所有神经元的值计算完毕。最后，将输出层每个神经元的值串在一起就得到了输出向量$\\overrightarrow{y}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.举例说明\n",
    "![images](images/02_00_02_005.png)<br/>\n",
    "如上图，输入层有三个节点，我们将其依次编号为1、2、3；隐藏层的4个节点，编号依次为4、5、6、7；最后输出层的两个节点编号为8、9。因为我们这个神经网络是全连接网络，所以可以看到每个节点都和上一层的所有节点有连接。比如，我们可以看到隐藏层的节点4，它和输入层的三个节点1、2、3之间都有连接，其连接上的权重分别为$\\omega_{41},\\omega_{42},\\omega_{43}$。那么，我们怎样计算节点4的输出值$\\alpha_4$呢？为了计算节点4的输出值，我们必须先得到其所有上游节点（也就是节点1、2、3）的输出值。节点1、2、3是输入层的节点，所以，他们的输出值就是输入向量$\\overrightarrow{x}$本身。按照上图画出的对应关系，可以看到节点1、2、3的输出值分别是$x_1,x_2,x_3$。我们要求输入向量的维度和输入层神经元个数相同，而输入向量的某个元素对应到哪个输入节点是可以自由决定的，你偏非要把$x_1$赋值给节点2也是完全没有问题的，但这样除了把自己弄晕之外，并没有什么价\n",
    "值。这样我们就可以计算出\n",
    "$$\\alpha_4 = sigmoid(\\overrightarrow{\\omega}^T \\bullet \\overrightarrow{x})=sigmoid(\\omega_{41}x_1+\\omega_{42}x_2+\\omega_{43}x_3+\\omega_{4b})$$\n",
    "同样，我们可以继续计算出节点5、6、7的输出值$\\alpha_5,\\alpha_6,\\alpha_7$。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以\n",
    "接着计算输出层的节点8的输出值\n",
    "$$y_1=sigmoid(\\overrightarrow{\\omega}^T \\bullet \\overrightarrow{x})=sigmoid(\\omega_{84}\\alpha_4+\\omega_{85}\\alpha_5+\\omega_{86}\\alpha_6+\\omega_{87}\\alpha_7+\\omega_{8b})$$\n",
    "同理，我们还可以计算出$y_2$的值。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量\n",
    "$\\overrightarrow{x}=\\begin{bmatrix}\n",
    "x_1\\\\\\\\\n",
    "x_2\\\\\\\\\n",
    "x_3\n",
    "\\end{bmatrix}$时，神经网络的输出向量$\\overrightarrow{y}=\\begin{bmatrix}\n",
    "y_1\\\\\\\\\n",
    "y_2\n",
    "\\end{bmatrix}$。这里我们也看到，输出向量的维度和输出层神经元个数相同。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
